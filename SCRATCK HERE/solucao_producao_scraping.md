# SOLU√á√ÉO PARA PRODU√á√ÉO: SISTEMA DE SCRAPING ESCAL√ÅVEL

## üèóÔ∏è ARQUITETURA RECOMENDADA

### 1. **SISTEMA DE FILAS COM REDIS/CELERY**

```python
# requirements.txt
celery==5.3.0
redis==4.6.0
flower==2.0.1  # Monitoramento

# celery_config.py
from celery import Celery

app = Celery('scraping_tasks')
app.config_from_object('celery_settings')

@app.task(bind=True, max_retries=3)
def scrape_os_task(self, numero_os, user_id):
    """Task ass√≠ncrona para scraping de OS"""
    try:
        # Executar scraping
        result = execute_scraping(numero_os)
        
        # Salvar no banco
        save_scraped_data(result, numero_os)
        
        # Notificar usu√°rio via WebSocket
        notify_user(user_id, "success", result)
        
        return {"status": "success", "data": result}
        
    except Exception as e:
        # Retry autom√°tico
        if self.request.retries < 3:
            raise self.retry(countdown=60 * (2 ** self.request.retries))
        
        # Notificar erro
        notify_user(user_id, "error", str(e))
        return {"status": "error", "message": str(e)}
```

### 2. **ENDPOINT ASS√çNCRONO**

```python
# routes/desenvolvimento.py
@router.post("/buscar-os-async/{numero_os}")
async def buscar_os_async(numero_os: str, current_user: Usuario = Depends(get_current_user)):
    """Inicia scraping ass√≠ncrono"""
    
    # 1. Verificar se OS j√° existe
    os_existente = db.query(OrdemServico).filter(
        OrdemServico.os_numero == numero_os
    ).first()
    
    if os_existente:
        return {"status": "found", "data": format_os_data(os_existente)}
    
    # 2. Verificar se j√° est√° na fila
    task_id = f"scraping_{numero_os}"
    existing_task = celery_app.AsyncResult(task_id)
    
    if existing_task.state == "PENDING":
        return {
            "status": "queued", 
            "message": "OS j√° est√° sendo processada",
            "task_id": task_id,
            "position": get_queue_position(task_id)
        }
    
    # 3. Adicionar √† fila
    task = scrape_os_task.apply_async(
        args=[numero_os, current_user.id],
        task_id=task_id,
        priority=get_user_priority(current_user)
    )
    
    return {
        "status": "queued",
        "message": "OS adicionada √† fila de processamento",
        "task_id": task.id,
        "estimated_time": get_estimated_time()
    }

@router.get("/scraping-status/{task_id}")
async def get_scraping_status(task_id: str):
    """Verifica status do scraping"""
    task = celery_app.AsyncResult(task_id)
    
    return {
        "status": task.state,
        "result": task.result,
        "progress": task.info.get('progress', 0) if task.info else 0
    }
```

### 3. **WORKERS DEDICADOS**

```bash
# Iniciar workers Celery
celery -A celery_config worker --loglevel=info --concurrency=2 --hostname=scraping-worker-1
celery -A celery_config worker --loglevel=info --concurrency=2 --hostname=scraping-worker-2

# Monitoramento
celery -A celery_config flower --port=5555
```

### 4. **FRONTEND COM WEBSOCKETS**

```typescript
// ScrapingService.ts
class ScrapingService {
    private socket: WebSocket;
    
    async buscarOSAsync(numeroOS: string): Promise<ScrapingResponse> {
        // 1. Iniciar scraping
        const response = await fetch(`/api/buscar-os-async/${numeroOS}`, {
            method: 'POST'
        });
        
        const data = await response.json();
        
        if (data.status === 'found') {
            return data;
        }
        
        // 2. Conectar WebSocket para updates
        this.connectWebSocket(data.task_id);
        
        // 3. Mostrar progresso
        this.showProgressModal(data);
        
        return data;
    }
    
    private connectWebSocket(taskId: string) {
        this.socket = new WebSocket(`ws://localhost:8000/ws/scraping/${taskId}`);
        
        this.socket.onmessage = (event) => {
            const data = JSON.parse(event.data);
            this.updateProgress(data);
        };
    }
    
    private updateProgress(data: any) {
        if (data.status === 'success') {
            this.showSuccess(data.result);
        } else if (data.status === 'error') {
            this.showError(data.message);
        } else {
            this.updateProgressBar(data.progress);
        }
    }
}
```

## üéØ **CONFIGURA√á√ïES DE PRODU√á√ÉO**

### **1. LIMITA√á√ÉO DE RECURSOS**
```python
# settings.py
SCRAPING_CONFIG = {
    "MAX_CONCURRENT_TASKS": 3,  # M√°ximo 3 scrapings simult√¢neos
    "TASK_TIMEOUT": 300,        # 5 minutos timeout
    "RETRY_DELAY": 60,          # 1 minuto entre tentativas
    "MAX_RETRIES": 3,           # M√°ximo 3 tentativas
    "RATE_LIMIT": "10/hour",    # 10 scrapings por hora por usu√°rio
}
```

### **2. CACHE INTELIGENTE**
```python
# Cache de OSs por 24h
@cache.memoize(timeout=86400)
def get_os_data(numero_os: str):
    return db.query(OrdemServico).filter(
        OrdemServico.os_numero == numero_os
    ).first()

# Cache de resultados de scraping por 1h
@cache.memoize(timeout=3600)
def get_scraping_result(numero_os: str):
    return execute_scraping(numero_os)
```

### **3. MONITORAMENTO**
```python
# M√©tricas de scraping
SCRAPING_METRICS = {
    "total_requests": 0,
    "successful_scrapings": 0,
    "failed_scrapings": 0,
    "average_time": 0,
    "queue_size": 0,
    "active_workers": 0
}

# Alertas autom√°ticos
def check_scraping_health():
    if SCRAPING_METRICS["queue_size"] > 50:
        send_alert("Fila de scraping muito grande")
    
    if SCRAPING_METRICS["failed_scrapings"] > 10:
        send_alert("Muitas falhas no scraping")
```

## üìä **BENEF√çCIOS DA SOLU√á√ÉO**

### ‚úÖ **ESCALABILIDADE**
- **Fila organizada** - Usu√°rios n√£o esperam travados
- **Workers dedicados** - Controle de recursos
- **Prioriza√ß√£o** - Usu√°rios VIP primeiro

### ‚úÖ **PERFORMANCE**
- **M√°ximo 3 scrapings simult√¢neos** - N√£o sobrecarrega
- **Cache inteligente** - Evita scraping desnecess√°rio
- **Timeout controlado** - N√£o trava indefinidamente

### ‚úÖ **EXPERI√äNCIA DO USU√ÅRIO**
- **Feedback em tempo real** - Progresso vis√≠vel
- **N√£o bloqueia interface** - Usu√°rio pode continuar trabalhando
- **Notifica√ß√µes** - Aviso quando conclu√≠do

### ‚úÖ **CONFIABILIDADE**
- **Retry autom√°tico** - Falhas tempor√°rias resolvidas
- **Monitoramento** - Problemas detectados rapidamente
- **Logs detalhados** - Debug facilitado

## üöÄ **IMPLEMENTA√á√ÉO GRADUAL**

### **FASE 1: B√ÅSICO (1 semana)**
- Implementar fila Redis/Celery
- Endpoint ass√≠ncrono b√°sico
- Frontend com polling

### **FASE 2: AVAN√áADO (2 semanas)**
- WebSockets para tempo real
- Cache inteligente
- Monitoramento b√°sico

### **FASE 3: PRODU√á√ÉO (1 semana)**
- M√©tricas completas
- Alertas autom√°ticos
- Otimiza√ß√µes finais

## üí∞ **CUSTO vs BENEF√çCIO**

### **SEM SOLU√á√ÉO:**
- ‚ùå Sistema inutiliz√°vel com 100+ usu√°rios
- ‚ùå Servidor constantemente travado
- ‚ùå Usu√°rios frustrados
- ‚ùå Perda de produtividade

### **COM SOLU√á√ÉO:**
- ‚úÖ Sistema est√°vel para 1000+ usu√°rios
- ‚úÖ Recursos controlados
- ‚úÖ Experi√™ncia fluida
- ‚úÖ Produtividade m√°xima

**INVESTIMENTO:** ~40 horas desenvolvimento
**RETORNO:** Sistema utiliz√°vel em produ√ß√£o
